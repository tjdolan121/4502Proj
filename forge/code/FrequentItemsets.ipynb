{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std lib:\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# apriori algorithm lib\n",
    "from apyori import apriori\n",
    "\n",
    "# To hide environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data manipulation:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "style.use('seaborn')\n",
    "\n",
    "# Display all columns in Jupyter:\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Filter Warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes = {'block': 'object',\n",
    "             'iucr': 'object',\n",
    "             'primary_type': 'object',\n",
    "             'description': 'object',\n",
    "             'location_description': 'object',\n",
    "             'arrest': 'bool',\n",
    "             'domestic': 'bool',\n",
    "             'beat': 'int64',\n",
    "             'district': 'float64',\n",
    "             'ward': 'float64',\n",
    "             'community_area': 'int64',\n",
    "             'fbi_code': 'object',\n",
    "             'x_coordinate': 'float64',\n",
    "             'y_coordinate': 'float64',\n",
    "             'year': 'int64',\n",
    "             'latitude': 'float64',\n",
    "             'longitude': 'float64',\n",
    "             'location': 'object',\n",
    "             'month': 'uint8',\n",
    "             'hour': 'uint8',\n",
    "             'dayofweek': 'uint8',\n",
    "             'weekend': 'bool',\n",
    "             'CF': 'float64',\n",
    "             'CF3': 'float64',\n",
    "             'PI': 'float64',\n",
    "             'EUH': 'float64',\n",
    "             'CH': 'float64',\n",
    "             'SOI': 'float64',\n",
    "             'crime_code_category': 'category',\n",
    "             'index_crime': 'bool',\n",
    "             'violent_crime': 'bool',\n",
    "             'property_crime': 'bool',\n",
    "             'crime_against_persons': 'bool',\n",
    "             'crime_against_property': 'bool',\n",
    "             'crime_against_society': 'bool'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.5 s, sys: 4.82 s, total: 42.3 s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_dotenv()\n",
    "path_to_data = os.environ.get('CLEAN_DATA')\n",
    "df = pd.read_csv(path_to_data, dtype=datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out attributes that will not be considered for frequent itemset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [ \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\", \\\n",
    "              \"location_description\", \"description\", \"crime_code_category\", \"arrest\", \\\n",
    "              \"domestic\", \"district\", \"ward\", \"community_area\", \"index_crime\", \"beat\", \\\n",
    "              \"violent_crime\", \"month\", \"hour\", \"dayofweek\", \"weekend\", \"block\", \"year\"]\n",
    "\n",
    "data = df[attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove extraneous address characters from 'block' attribute to validate its use in frequent itemset mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 160 ms, total: 2.15 s\n",
      "Wall time: 2.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                        E 84TH ST\n",
       "1                        W 57TH ST\n",
       "2                         S MAY ST\n",
       "3                  S BALTIMORE AVE\n",
       "4    S DR MARTIN LUTHER KING JR DR\n",
       "Name: block, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data['block'] = data['block'].str[6:]\n",
    "data['block'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert column values to numbers to speed up frequent itemset mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 24s, sys: 1min 6s, total: 18min 30s\n",
      "Wall time: 18min 46s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime_against_persons</th>\n",
       "      <th>crime_against_property</th>\n",
       "      <th>crime_against_society</th>\n",
       "      <th>location_description</th>\n",
       "      <th>description</th>\n",
       "      <th>crime_code_category</th>\n",
       "      <th>arrest</th>\n",
       "      <th>domestic</th>\n",
       "      <th>district</th>\n",
       "      <th>ward</th>\n",
       "      <th>community_area</th>\n",
       "      <th>index_crime</th>\n",
       "      <th>beat</th>\n",
       "      <th>violent_crime</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekend</th>\n",
       "      <th>block</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>220</td>\n",
       "      <td>748</td>\n",
       "      <td>774</td>\n",
       "      <td>776</td>\n",
       "      <td>778.0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>852</td>\n",
       "      <td>930</td>\n",
       "      <td>932</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>1249</td>\n",
       "      <td>1273</td>\n",
       "      <td>1280</td>\n",
       "      <td>1282</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>221</td>\n",
       "      <td>748</td>\n",
       "      <td>775</td>\n",
       "      <td>776</td>\n",
       "      <td>779.0</td>\n",
       "      <td>803.0</td>\n",
       "      <td>853</td>\n",
       "      <td>930</td>\n",
       "      <td>933</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>1250</td>\n",
       "      <td>1274</td>\n",
       "      <td>1280</td>\n",
       "      <td>1283</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>221</td>\n",
       "      <td>748</td>\n",
       "      <td>775</td>\n",
       "      <td>776</td>\n",
       "      <td>780.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>854</td>\n",
       "      <td>930</td>\n",
       "      <td>934</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>1251</td>\n",
       "      <td>1275</td>\n",
       "      <td>1280</td>\n",
       "      <td>1284</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>220</td>\n",
       "      <td>748</td>\n",
       "      <td>774</td>\n",
       "      <td>776</td>\n",
       "      <td>778.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>852</td>\n",
       "      <td>930</td>\n",
       "      <td>935</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>1252</td>\n",
       "      <td>1275</td>\n",
       "      <td>1280</td>\n",
       "      <td>1285</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>221</td>\n",
       "      <td>748</td>\n",
       "      <td>774</td>\n",
       "      <td>777</td>\n",
       "      <td>781.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>855</td>\n",
       "      <td>930</td>\n",
       "      <td>936</td>\n",
       "      <td>1235</td>\n",
       "      <td>1237</td>\n",
       "      <td>1253</td>\n",
       "      <td>1276</td>\n",
       "      <td>1281</td>\n",
       "      <td>1286</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   crime_against_persons  crime_against_property  crime_against_society  \\\n",
       "0                      0                       2                      4   \n",
       "1                      0                       2                      4   \n",
       "2                      0                       2                      4   \n",
       "3                      0                       2                      4   \n",
       "4                      0                       2                      4   \n",
       "\n",
       "   location_description  description  crime_code_category  arrest  domestic  \\\n",
       "0                     6          220                  748     774       776   \n",
       "1                     7          221                  748     775       776   \n",
       "2                     8          221                  748     775       776   \n",
       "3                     7          220                  748     774       776   \n",
       "4                     9          221                  748     774       777   \n",
       "\n",
       "   district   ward  community_area  index_crime  beat  violent_crime  month  \\\n",
       "0     778.0  802.0             852          930   932           1235   1237   \n",
       "1     779.0  803.0             853          930   933           1235   1237   \n",
       "2     780.0  804.0             854          930   934           1235   1237   \n",
       "3     778.0  805.0             852          930   935           1235   1237   \n",
       "4     781.0  806.0             855          930   936           1235   1237   \n",
       "\n",
       "   hour  dayofweek  weekend  block  year  \n",
       "0  1249       1273     1280   1282  4577  \n",
       "1  1250       1274     1280   1283  4577  \n",
       "2  1251       1275     1280   1284  4577  \n",
       "3  1252       1275     1280   1285  4577  \n",
       "4  1253       1276     1281   1286  4577  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "###### BEWARE -- 12+ min to run this block. \n",
    "\n",
    "data_num = data.copy()\n",
    "val_dict = {}\n",
    "index = 0\n",
    "\n",
    "for attribute in attributes:\n",
    "    # Important! Reset so column vals don't cross-pollinate:\n",
    "    num_dict = {}\n",
    "    \n",
    "    # Get all unique values for column:\n",
    "    values = pd.unique(data[attribute])\n",
    "    \n",
    "    # For each unique value replace with an index no.\n",
    "    for value in values:\n",
    "        # add index no. as key to value for later reference:\n",
    "        val_dict[index] = (attribute, value)\n",
    "        num_dict[value] = index\n",
    "        index += 1\n",
    "    # Replace per column to avoid cross-pollination.\n",
    "    data_num[attribute] = data_num[attribute].replace(num_dict)\n",
    "    \n",
    "######\n",
    "\n",
    "# verify results\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the apyori library, we will need to be able to convert dataframes into list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_2_list(df):\n",
    "    return df.to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As also need a means of formatting and returning our results as a dataframe with human-readable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_results(association_results):\n",
    "    \n",
    "    dataframes = []\n",
    "    for rule in association_results:\n",
    "        data = {}\n",
    "        \n",
    "        # create a rule from results\n",
    "        base = list(rule[2][0][0])\n",
    "        base = [val_dict[b][1] for b in base]\n",
    "        add = list(rule[2][0][1])\n",
    "        add = val_dict[add[0]][1]\n",
    "        data['rule'] = str(base) + ' ==> ' + str(add)\n",
    "\n",
    "        # support, confidence, and lift of rule\n",
    "        data['lift'] = rule[2][0][3]\n",
    "        data['confidence'] = rule[2][0][2]\n",
    "        data['support'] = rule[1]\n",
    "\n",
    "        # components of rule\n",
    "        for item in rule[0]:\n",
    "            data[val_dict[item][0]] = val_dict[item][1]\n",
    "        \n",
    "        # convert rules to dataframe rows\n",
    "        dataframe = pd.DataFrame.from_records([data])\n",
    "        dataframes.append(dataframe)\n",
    "    \n",
    "        # concat rules as rows of single dataframe\n",
    "    return pd.concat(dataframes, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we want a wrapper function that converts the dataframe, applies the apriori class, and converts the results to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_apriori(df, supp, conf, lift):\n",
    "    records = convert_df_2_list(df)\n",
    "    association_rules = apriori(records, min_support=supp, min_confidence=conf, min_lift=lift)\n",
    "    association_results = list(association_rules)\n",
    "    return df_results(association_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CITATION:\n",
    "Regarding use of the apyori lib see our references here:\n",
    "- https://github.com/ymoch/apyori\n",
    "- https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/\n",
    "- pydoc entry:<br>\n",
    "    ```\n",
    "    apyori.apriori = apriori(transactions, **kwargs)\n",
    "        Executes Apriori algorithm and returns a RelationRecord generator.\n",
    "\n",
    "        Arguments:\n",
    "            transactions -- A transaction iterable object\n",
    "                            (eg. [['A', 'B'], ['B', 'C']]).\n",
    "\n",
    "        Keyword arguments:\n",
    "            min_support -- The minimum support of relations (float).\n",
    "            min_confidence -- The minimum confidence of relations (float).\n",
    "            min_lift -- The minimum lift of relations (float).\n",
    "            max_length -- The maximum length of the relation (integer).\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify interesting subsets of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have chosen twenty attributes to mine for frequent itemsets, many of which have vary in form and frequency, it would be impractical to mine them all at once. Instead we need to select subsets of our attributes. However, with twenty total attributes we have 1,048,575 possible combinations. So, we need to be selective. \n",
    "<br><br>\n",
    "There are several ways we can look at our attributes when deciding which to combine. One is to look at the the number of unique values each attribute contains. This has the added benefit of helping us later when we are setting the minimum support, confidence, and lift values as parameters to the apriori algorithm. Even better, we can also take a look at how often the two least frequent unique values occur, which will give us a major clue how low we may need to lower the minimum support in order for certain unique values to not be automatically omitted from our results. Given that we have 6,508,416 rows of data total, if some of these attributes have very low frequency counts for unique values, then that means we may need to work our way down to very low support levels for certain columns in order to reveal certain frequent itemsets that may still have high confidence and lift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique vals</th>\n",
       "      <th>count least frequent</th>\n",
       "      <th>% of total</th>\n",
       "      <th>count 2nd least</th>\n",
       "      <th>2nd % of total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attributes</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>crime_against_persons</th>\n",
       "      <td>2</td>\n",
       "      <td>1713049</td>\n",
       "      <td>26.3205%</td>\n",
       "      <td>4795367</td>\n",
       "      <td>73.6795%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime_against_property</th>\n",
       "      <td>2</td>\n",
       "      <td>3200474</td>\n",
       "      <td>49.1744%</td>\n",
       "      <td>3307942</td>\n",
       "      <td>50.8256%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime_against_society</th>\n",
       "      <td>2</td>\n",
       "      <td>1543323</td>\n",
       "      <td>23.7127%</td>\n",
       "      <td>4965093</td>\n",
       "      <td>76.2873%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekend</th>\n",
       "      <td>2</td>\n",
       "      <td>1817210</td>\n",
       "      <td>27.9209%</td>\n",
       "      <td>4691206</td>\n",
       "      <td>72.0791%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrest</th>\n",
       "      <td>2</td>\n",
       "      <td>1762303</td>\n",
       "      <td>27.0773%</td>\n",
       "      <td>4746113</td>\n",
       "      <td>72.9227%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>2</td>\n",
       "      <td>881185</td>\n",
       "      <td>13.5392%</td>\n",
       "      <td>5627231</td>\n",
       "      <td>86.4608%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violent_crime</th>\n",
       "      <td>2</td>\n",
       "      <td>565845</td>\n",
       "      <td>8.6941%</td>\n",
       "      <td>5942571</td>\n",
       "      <td>91.3059%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_crime</th>\n",
       "      <td>2</td>\n",
       "      <td>2622479</td>\n",
       "      <td>40.2937%</td>\n",
       "      <td>3885937</td>\n",
       "      <td>59.7063%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dayofweek</th>\n",
       "      <td>7</td>\n",
       "      <td>884921</td>\n",
       "      <td>13.5966%</td>\n",
       "      <td>918481</td>\n",
       "      <td>14.1122%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>12</td>\n",
       "      <td>430112</td>\n",
       "      <td>6.6086%</td>\n",
       "      <td>483102</td>\n",
       "      <td>7.4227%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>20</td>\n",
       "      <td>3883</td>\n",
       "      <td>0.0597%</td>\n",
       "      <td>138906</td>\n",
       "      <td>2.1343%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0001%</td>\n",
       "      <td>153</td>\n",
       "      <td>0.0024%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>24</td>\n",
       "      <td>88639</td>\n",
       "      <td>1.3619%</td>\n",
       "      <td>104411</td>\n",
       "      <td>1.6042%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime_code_category</th>\n",
       "      <td>26</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0008%</td>\n",
       "      <td>1160</td>\n",
       "      <td>0.0178%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ward</th>\n",
       "      <td>50</td>\n",
       "      <td>57562</td>\n",
       "      <td>0.8844%</td>\n",
       "      <td>60692</td>\n",
       "      <td>0.9325%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>community_area</th>\n",
       "      <td>78</td>\n",
       "      <td>67</td>\n",
       "      <td>0.001%</td>\n",
       "      <td>6285</td>\n",
       "      <td>0.0966%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_description</th>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beat</th>\n",
       "      <td>303</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0003%</td>\n",
       "      <td>1012</td>\n",
       "      <td>0.0155%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>528</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>block</th>\n",
       "      <td>3295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        unique vals  count least frequent % of total  \\\n",
       "attributes                                                             \n",
       "crime_against_persons             2               1713049   26.3205%   \n",
       "crime_against_property            2               3200474   49.1744%   \n",
       "crime_against_society             2               1543323   23.7127%   \n",
       "weekend                           2               1817210   27.9209%   \n",
       "arrest                            2               1762303   27.0773%   \n",
       "domestic                          2                881185   13.5392%   \n",
       "violent_crime                     2                565845    8.6941%   \n",
       "index_crime                       2               2622479   40.2937%   \n",
       "dayofweek                         7                884921   13.5966%   \n",
       "month                            12                430112    6.6086%   \n",
       "year                             20                  3883    0.0597%   \n",
       "district                         24                     4    0.0001%   \n",
       "hour                             24                 88639    1.3619%   \n",
       "crime_code_category              26                    54    0.0008%   \n",
       "ward                             50                 57562    0.8844%   \n",
       "community_area                   78                    67     0.001%   \n",
       "location_description            214                     1       0.0%   \n",
       "beat                            303                    17    0.0003%   \n",
       "description                     528                     1       0.0%   \n",
       "block                          3295                     1       0.0%   \n",
       "\n",
       "                        count 2nd least 2nd % of total  \n",
       "attributes                                              \n",
       "crime_against_persons           4795367       73.6795%  \n",
       "crime_against_property          3307942       50.8256%  \n",
       "crime_against_society           4965093       76.2873%  \n",
       "weekend                         4691206       72.0791%  \n",
       "arrest                          4746113       72.9227%  \n",
       "domestic                        5627231       86.4608%  \n",
       "violent_crime                   5942571       91.3059%  \n",
       "index_crime                     3885937       59.7063%  \n",
       "dayofweek                        918481       14.1122%  \n",
       "month                            483102        7.4227%  \n",
       "year                             138906        2.1343%  \n",
       "district                            153        0.0024%  \n",
       "hour                             104411        1.6042%  \n",
       "crime_code_category                1160        0.0178%  \n",
       "ward                              60692        0.9325%  \n",
       "community_area                     6285        0.0966%  \n",
       "location_description                  1           0.0%  \n",
       "beat                               1012        0.0155%  \n",
       "description                           1           0.0%  \n",
       "block                                 1           0.0%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unq, frq1, frq2, perc1, perc2 = [], [], [], [], []\n",
    "for attribute in attributes:\n",
    "    unq.append(len(pd.unique(data[attribute])))\n",
    "    f1 = data[attribute].value_counts(ascending=True).iloc[0]\n",
    "    frq1.append(f1)\n",
    "    f2 = data[attribute].value_counts(ascending=True).iloc[1]\n",
    "    frq2.append(f2)\n",
    "    perc1.append(str(round((f1 / 6508416)*100,4)) + '%')\n",
    "    perc2.append(str(round((f2 / 6508416)*100,4)) + '%')\n",
    "display(pd.DataFrame({'attributes': attributes, 'unique vals': unq, \\\n",
    "                      'count least frequent': frq1, \\\n",
    "                      '% of total': perc1, \\\n",
    "                      'count 2nd least': frq2, \\\n",
    "                      '2nd % of total': perc2} \\\n",
    "                    ).set_index('attributes').sort_values(by=['unique vals']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, another way we can also look at the different attributes to decide what combinations will be most useful is in terms of what information they provide given the context. By doing that, we can roughly divide them into three groups: the WHAT, WHERE, and WHEN of a crime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| WHAT | WHERE | WHEN |\n",
    "|:------|:-------|:------|\n",
    "|crime_against_persons|district|month|\n",
    "|crime_against_property|ward|hour|\n",
    "|description|community_area|dayofweek|\n",
    "|location_description|beat|weekend|\n",
    "|crime_code_category|block|year|\n",
    "|arrest|||\n",
    "|domestic|||\n",
    "|index_crime|||\n",
    "|violent_crime|||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this information in hand, we can make some more informed decisions about what subsets will be the most salient and/or provide the best results due to similarities in the unique value counts and contextual differences and similarities. We will want to avoid including attributes that are too similar to each other, or otherwise overlap, while also avoid using attributes together that are too different in terms of their unique items counts.<br>\n",
    "<br>\n",
    "Using these criteria to narrow data subsets down, we define the list of candidates we will consider here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create dataframe subsets for frequent itemset mining\n",
    "\n",
    "bk_dn = data_num[[\"block\", \"description\"]]\n",
    "bk_ld = data_num[[\"block\", \"location_description\"]]\n",
    "bt_dn = data_num[[\"beat\", \"description\"]]\n",
    "bt_ld = data_num[[\"beat\", \"location_description\"]]\n",
    "ld_dn = data_num[[\"description\", \"location_description\"]]\n",
    "ca_dn = data_num[[\"community_area\", \"description\"]]\n",
    "ca_ld = data_num[[\"community_area\", \"location_description\"]]\n",
    "ca_hr = data_num[[\"community_area\", \"hour\"]]\n",
    "ld_ca_ccc = data_num[[\"community_area\", \"location_description\", \"crime_code_category\"]]\n",
    "ca_cape_capo_cas = data_num[[\"community_area\", \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\"]]\n",
    "ld_cape_capo_cas = data_num[[\"location_description\", \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\"]]\n",
    "wkn_cape_capo_cas = data_num[[\"weekend\", \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\"]]\n",
    "hr_cape_capo_cas = data_num[[\"hour\", \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\"]]\n",
    "arr_dom_idx_vc = data_num[[\"arrest\", \"domestic\", \"index_crime\", \"violent_crime\"]]\n",
    "ca_cape_capo_cas_arr = data_num[[\"community_area\", \"crime_against_persons\", \"crime_against_property\", \"crime_against_society\", \"arrest\"]]\n",
    "ca_arr_dom_idx_vc = data_num[[\"community_area\", \"arrest\", \"domestic\", \"index_crime\", \"violent_crime\"]]\n",
    "ccc_hr_yr_mon_dow_dc = data_num[[\"crime_code_category\", \"hour\", \"year\", \"month\", \"dayofweek\", \"district\"]]\n",
    "ccc_hr_yr_mon_dow_wd = data_num[[\"crime_code_category\", \"hour\", \"year\", \"month\", \"dayofweek\", \"ward\"]]\n",
    "ccc_hr_yr_mon_dow_ca = data_num[[\"crime_code_category\", \"hour\", \"year\", \"month\", \"dayofweek\", \"community_area\"]]\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Apriori\n",
    "Finally, with the subsets of data we want to explore the most prepared, we can begin iteratively applying our application of the apriori alogorithm, adjusting the parameters as we go, in pursuit of interesting information. The final forumula for applying the apriori to a subset of the data is provided here, using the ld_ca_ccc data subset as our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 rules found.\n",
      "\n",
      "CPU times: user 23.6 s, sys: 1.3 s, total: 24.9 s\n",
      "Wall time: 25.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule</th>\n",
       "      <th>lift</th>\n",
       "      <th>confidence</th>\n",
       "      <th>support</th>\n",
       "      <th>crime_code_category</th>\n",
       "      <th>location_description</th>\n",
       "      <th>community_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['RESIDENCE-GARAGE'] ==&gt; Burglary</td>\n",
       "      <td>8.856286</td>\n",
       "      <td>0.505816</td>\n",
       "      <td>0.009495</td>\n",
       "      <td>Burglary</td>\n",
       "      <td>RESIDENCE-GARAGE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[25, 'SIDEWALK'] ==&gt; Drug Abuse</td>\n",
       "      <td>4.930013</td>\n",
       "      <td>0.468830</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>Drug Abuse</td>\n",
       "      <td>SIDEWALK</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['DEPARTMENT STORE'] ==&gt; Larceny</td>\n",
       "      <td>3.866943</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>Larceny</td>\n",
       "      <td>DEPARTMENT STORE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Prostitution'] ==&gt; STREET</td>\n",
       "      <td>3.197930</td>\n",
       "      <td>0.819378</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>Prostitution</td>\n",
       "      <td>STREET</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['SIDEWALK'] ==&gt; Drug Abuse</td>\n",
       "      <td>3.182014</td>\n",
       "      <td>0.302601</td>\n",
       "      <td>0.030520</td>\n",
       "      <td>Drug Abuse</td>\n",
       "      <td>SIDEWALK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Robbery'] ==&gt; SIDEWALK</td>\n",
       "      <td>3.149042</td>\n",
       "      <td>0.317610</td>\n",
       "      <td>0.011989</td>\n",
       "      <td>Robbery</td>\n",
       "      <td>SIDEWALK</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Motor Vehicle Theft'] ==&gt; STREET</td>\n",
       "      <td>3.074678</td>\n",
       "      <td>0.787798</td>\n",
       "      <td>0.035736</td>\n",
       "      <td>Motor Vehicle Theft</td>\n",
       "      <td>STREET</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['GROCERY FOOD STORE'] ==&gt; Larceny</td>\n",
       "      <td>3.025541</td>\n",
       "      <td>0.641142</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>Larceny</td>\n",
       "      <td>GROCERY FOOD STORE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rule      lift  confidence   support  \\\n",
       "6   ['RESIDENCE-GARAGE'] ==> Burglary  8.856286    0.505816  0.009495   \n",
       "7     [25, 'SIDEWALK'] ==> Drug Abuse  4.930013    0.468830  0.005493   \n",
       "5    ['DEPARTMENT STORE'] ==> Larceny  3.866943    0.819444  0.010368   \n",
       "3         ['Prostitution'] ==> STREET  3.197930    0.819378  0.007665   \n",
       "0         ['SIDEWALK'] ==> Drug Abuse  3.182014    0.302601  0.030520   \n",
       "1            ['Robbery'] ==> SIDEWALK  3.149042    0.317610  0.011989   \n",
       "2  ['Motor Vehicle Theft'] ==> STREET  3.074678    0.787798  0.035736   \n",
       "4  ['GROCERY FOOD STORE'] ==> Larceny  3.025541    0.641142  0.008058   \n",
       "\n",
       "   crime_code_category location_description  community_area  \n",
       "6             Burglary     RESIDENCE-GARAGE             NaN  \n",
       "7           Drug Abuse             SIDEWALK            25.0  \n",
       "5              Larceny     DEPARTMENT STORE             NaN  \n",
       "3         Prostitution               STREET             NaN  \n",
       "0           Drug Abuse             SIDEWALK             NaN  \n",
       "1              Robbery             SIDEWALK             NaN  \n",
       "2  Motor Vehicle Theft               STREET             NaN  \n",
       "4              Larceny   GROCERY FOOD STORE             NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ld_ca_ccc\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "demo_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(demo_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "demo_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Applying the apriori algorithm is an extremely iterative process. Besides the many, many different subsets of data we can choose from, there is an almost infinite number of tweaks that can be made to the minimum support, minimum confidence, and minimum lift parameters for each one. Each results must then be interpreted by human user to analyze the validity, usefulness, and interestingness. Furthermore, it must be noted that there will always be inherent biases in the data, hence there will be biases in the results. Whether that bias is due to some information that is missing, or bias in how the information itself was originally gathered, we cannot take any results as a penultimate truth. The results shown below are therefore not exhaustive, and only meant to show some of the more interesting results we found and only a small sliver of what is possible. The parameters for each result provided below has been fine tuned for each case, but in each case other possibilities exist depending on the information that one wants to show. Copy and paste these into the Sandbox section to make adjustments and drill down on information you would like to explore further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### block, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = bk_dn\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "bk_dn_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(bk_dn_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#bk_dn_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### block, location_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = bk_ld\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "bk_ld_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(bk_ld_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#bk_ld_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beat, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = bt_dn\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "bt_dn_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(bt_dn_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#bt_dn_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beat, location_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = bt_ld\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "bt_ld_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(bt_ld_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#bt_ld_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location, location_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ld_dn\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ld_dn_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ld_dn_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ld_dn_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_dn\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_dn_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_dn_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_dn_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, location description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_ld\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_ld_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_ld_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_ld_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_hr\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_hr_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_hr_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_hr_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, location description, crime code category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ld_ca_ccc\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ld_ca_ccc_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ld_ca_ccc_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ld_ca_ccc_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, crime against persons, crime against property, crime against society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_cape_capo_cas\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_cape_capo_cas_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_cape_capo_cas_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_cape_capo_cas_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location description, crime against persons, crime against property, crime against society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ld_cape_capo_cas\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ld_cape_capo_cas_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ld_cape_capo_cas_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ld_cape_capo_cas_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weekend, crime against persons, crime against property, crime against society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = wkn_cape_capo_cas\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "wkn_cape_capo_cas_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(wkn_cape_capo_cas_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#wkn_cape_capo_cas_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hour, crime against persons, crime against property, crime against society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = hr_cape_capo_cas\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "hr_cape_capo_cas_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(hr_cape_capo_cas_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#hr_cape_capo_cas_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arrest, domestic crime, index crime (more serious crime), violent crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = arr_dom_idx_vc\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "arr_dom_idx_vc_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(arr_dom_idx_vc_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#arr_dom_idx_vc_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, crime against persons, crime against property, crime against society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_cape_capo_cas_arr\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_cape_capo_cas_arr_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_cape_capo_cas_arr_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_cape_capo_cas_arr_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### community area, arrest, domestic crime, index crime, violent crime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ca_arr_dom_idx_vc\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ca_arr_dom_idx_vc_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ca_arr_dom_idx_vc_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ca_arr_dom_idx_vc_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crime code category, hour, year, month, day of the week (integers), district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ccc_hr_yr_mon_dow_dc\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ccc_hr_yr_mon_dow_dc_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ccc_hr_yr_mon_dow_dc_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ccc_hr_yr_mon_dow_dc_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crime code category, hour, year, month, day of the week, ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ccc_hr_yr_mon_dow_wd\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ccc_hr_yr_mon_dow_wd_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ccc_hr_yr_mon_dow_wd_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ccc_hr_yr_mon_dow_wd_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crime code category, hour, year, month, day of week, community area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "####### final formula for frequent itemset mining\n",
    "\n",
    "# adjustable parameters:\n",
    "dataframe_to_analyze = ccc_hr_yr_mon_dow_ca\n",
    "minimum_support = 0.0045\n",
    "minimum_confidence = 0.2\n",
    "minimum_lift = 3\n",
    "\n",
    "# convert dataframe to consumable list of lists, apply apriori to identify frequent itemsets, and print results:\n",
    "ccc_hr_yr_mon_dow_ca_results = use_apriori(dataframe_to_analyze, minimum_support, minimum_confidence, minimum_lift)\n",
    "print(ccc_hr_yr_mon_dow_ca_results.shape[0], \"rules found.\")\n",
    "print()\n",
    "\n",
    "# display results\n",
    "#ccc_hr_yr_mon_dow_ca_results.sort_values(by=['lift', 'confidence', 'support'], ascending=False)\n",
    "\n",
    "#######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Given the wide range of results, it would be impossible to fully interpret the results we have here. However, we can highlight some of the more interesting observations. In some cases the reason behind what we have found is immediately apparent, and the results only serve to confirm things we already know. Some insights raise interesting questions that could, and perhaps should, be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work:\n",
    "Many of the results we have found can be built upon by downsampling the data to explore subsets of the data rows (not just columns) or by applying other data mining methodologies given the knowledge we obtained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SANDBOX\n",
    "Use this space to apply apriori method to any one of the data subsets we have provided or any that you would prefer to define yourself. Tweak the parameters to explore different results and make judgements on their validity based on what you know about the inputs as they relate to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
